{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Algorithmic Machine Learning Challenge</h3>\n",
    "<h1>Plankton Image Classification</h1>\n",
    "<hr style=\"height:2px;border:none;color:#333;background-color:#333;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plankton comprises all the organisms freely drifting with ocean currents. These life forms are a critically important piece of oceanic ecosystems, accounting for more than half the primary production on earth and nearly half the total carbon fixed in the global carbon cycle. They also form the foundation of aquatic food webs, including those of large, commercially important fisheries. Loss of plankton populations could result in ecological upheaval as well as negative societal impacts, particularly in indigenous cultures and the developing world. Plankton’s global significance makes their population levels an ideal measure of the health of the world’s oceans and ecosystems.\n",
    "\n",
    "Traditional methods for measuring and monitoring plankton populations are time consuming and cannot scale to the granularity or scope necessary for large-scale studies. Improved approaches are needed. One such approach is through the use of underwater imagery sensors. \n",
    "\n",
    "In this challenge, which was prepared in cooperation with the Laboratoire d’Océanographie de Villefranche, jointly run by Sorbonne Université and CNRS, plankton images were acquired in the bay of Villefranche, weekly since 2013 and manually engineered features were computed on each imaged object. \n",
    "\n",
    "This challenge aims at developing solid approaches to plankton image classification. We will compare methods based on carefully (but manually) engineered features, with “Deep Learning” methods in which features will be learned from image data alone.\n",
    "\n",
    "The purpose of this challenge is for you to learn about the commonly used paradigms when working with computer vision problems. This means you can choose one of the following paths:\n",
    "\n",
    "- Work directly with the provided images, e.g. using a (convolutional) neural network\n",
    "- Work with the supplied features extracted from the images (*native* or *skimage* or both of them)\n",
    "- Extract your own features from the provided images using a technique of your choice\n",
    "\n",
    "You will find a detailed description about the image data and the features at the end of this text.\n",
    "In any case, the choice of the classifier that you decide to work with strongly depends on the choice of features.\n",
    "\n",
    "Please bear in mind that the purpose of this challenge is not simply to find the best-performing model that was released on e.g. Kaggle for a similar problem. You should rather make sure to understand the dificulties that come with this computer vision task. Moreover, you should be able to justify your choice of features/model and be able to explain its advantages and disadvantages for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "<hr style=\"height:1px;border:none;color:#333;background-color:#333;\" />    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond simply producing a well-performing model for making predictions, in this challenge we would like you to start developing your skills as a machine learning scientist.\n",
    "In this regard, your notebook should be structured in such a way as to explore the five following tasks that are expected to be carried out whenever undertaking such a project.\n",
    "The description below each aspect should serve as a guide for your work, but you are strongly encouraged to also explore alternative options and directions. \n",
    "Thinking outside the box will always be rewarded in these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>1. Data Exploration</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first broad component of your notebook should enable you to familiarise yourselves with the given data, an outline of which is given at the end of this challenge specification.\n",
    "\n",
    "What is new in this challenge is that you will be working with image data. Therefore, you should have a look at example images located in the *imgs.zip* file (see description below). If you decide to work with the native or the skimage features, make sure to understand them!\n",
    "\n",
    "Among others, this section should investigate:\n",
    "\n",
    "- Distribution of the different image dimensions (including the number of channels)\n",
    "- Distribution of the different labels that the images are assigned to\n",
    "\n",
    "The image labels are organized in a taxonomy. We will measure the final model performance for the classification into the *level2* categories. Make sure to understand the meaning of this label inside the taxonomy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>2. Data Pre-processing</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous step should give you a better understanding of which pre-processing is required for the data based on your approach:\n",
    "\n",
    "- If you decide to work with the provided features, some data cleaning may be required to make full use of all the data.\n",
    "- If you decide to extract your own features from the images, you should explain your approach in this section.\n",
    "- If you decide to work directly with the images themselves, preprocessing the images may improve your classification results. In particular, if you work with a neural network the following should be of interest to you:\n",
    "\n",
    "  - Due to the fully-connected layers (that usually come after the convolutional ones), the input needs to have a fixed dimension.\n",
    "  - Data augmentation (image rotation, scaling, cropping, etc. of the existing images) can be used to increase the size of the training data set. This may improve performance especially when little data is available for a particular class.\n",
    "  - Be aware of the computational cost! It might be worth rescaling the images to a smaller size!\n",
    "\n",
    "  All of the operations above are usually realized using a dataloader. This means that you do not need to create a modified version of the dataset and save it to disk. Instead, the dataloader processes the data \"on the fly\" and in-memory before passing it to the network.\n",
    "  \n",
    "    NB: Although aligning image sizes is necessary to train CNNs, this will prevent your classifier from learning about different object sizes as a feature. Additional gains may be achieved when also taking object sizes into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>3. Model Selection</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most important segment of this challenge involves the selection of a model that can successfully handle the given data and yield sensible predictions.\n",
    "Instead of focusing exclusively on your final chosen model, it is also important to share your thought process in this notebook by additionally describing alternative candidate models.\n",
    "\n",
    "The choice of your model is closely connected to the way you preprocessed the input data.\n",
    "\n",
    "Furthermore, there are other factors which may influence your decision:\n",
    "\n",
    "- What is the model's complexity?\n",
    "- Is the model interpretable?\n",
    "- Is the model capable of handling different data-types?\n",
    "- Does the model return uncertainty estimates along with predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>4. Parameter Optimisation</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irrespective of your choice, it is highly likely that your model will have one or more (hyper-)parameters that require tuning.\n",
    "There are several techniques for carrying out such a procedure, including cross-validation, Bayesian optimisation, and several others.\n",
    "As before, an analysis into which parameter tuning technique best suits your model is expected before proceeding with the optimisation of your model.\n",
    "\n",
    "If you use a neural network, the optimization of hyperparameters (learning rate, weight decay, etc.) can be a very time-consuming process. In this case, your may decide to carry out smaller experiments and to justify your choice on these preliminary tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h3>5. Model Evaluation</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some form of pre-evaluation will inevitably be required in the preceding sections in order to both select an appropriate model and configure its parameters appropriately.\n",
    "In this final section, you may evaluate other aspects of the model such as:\n",
    "\n",
    "- Assessing the running time of your model;\n",
    "- Determining whether some aspects can be parallelised;\n",
    "- Training the model with smaller subsets of the data.\n",
    "- etc.\n",
    "\n",
    "For the evaluation of the classification results, you should use the F1 measure (see Submission Instructions). Here the focus should be on level2 classification. A classification evaluation for other labels is optional.\n",
    "\n",
    "Please note that you are responsible for creating a sensible train/validation/test split. There is no predefined held-out test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <b>N.B.</b> Please note that the items listed under each heading are neither exhaustive, nor are you expected to explore every given suggestion.\n",
    "    Nonetheless, these should serve as a guideline for your work in both this and upcoming challenges.\n",
    "    As always, you should use your intuition and understanding in order to decide which analysis best suits the assigned task.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h2>Submission Instructions</h2>\n",
    "    <hr style=\"height:1px;border:none;color:#333;background-color:#333;\" />    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal of this challenge is to construct a model for predicting Plankton (taxonomy level 2) classes.\n",
    "\n",
    "- Your submission will be the <b>HTML version of your notebook</b> exploring the various modelling aspects described above.\n",
    "\n",
    "- At the end of the notebook you should indicate your final evaluation score on a held-out test set. As an evaluation metric you should use the F1 score with the *average=macro* option as it is provided by the scikit-learn library. See the following link for more information:\n",
    "        \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"\">\n",
    "    <h2>Dataset Description</h2>\n",
    "    <hr style=\"height:1px;border:none;color:#333;background-color:#333;\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Location of the Dataset on zoe\n",
    "The data for this challenge is located at: `/mnt/datasets/plankton/flowcam`\n",
    "\n",
    "#### * Hierachical Taxonomy Tree for Labels \n",
    "\n",
    "Each object is represented by a single image and is identified by a unique integer number. It has a name associated to it which is integrated in a hierarchical taxonomic tree. The identifications are gathered from different projects, classified by different people in different contexts, so they often target different taxonomic levels. For example, let us say we classify items of clothing along the following tree\n",
    "\n",
    "    top\n",
    "        shirt\n",
    "            long sleeves\n",
    "            short sleeves\n",
    "        sweater\n",
    "            hooded\n",
    "            no hood\n",
    "    bottom\n",
    "        pants\n",
    "            jeans\n",
    "            other\n",
    "        shorts\n",
    "        \n",
    "In a first project, images are classified to the finest level possible, but it may be the case that, on some pictures, it is impossible to determine whether a sweater has a hood or not, in which case it is simply classified as `sweater`. In the second project, the operator classified tops as `shirt` or `sweater` only, and bottoms to the finest level. In a third project, the operator only separated tops from bottoms. In such a context, the original names in the database cannot be used directly because, for example `sweater` will contain images that are impossible to determine as `hooded` or `no hood` *as well as* `hooded` and `no hood` images that were simply not classified further. If all three classes (`sweater`, `hooded`, and `no hood`) are included in the training set, it will likely confuse the classifier. For this reason, we define different target taxonomic levels:\n",
    "\n",
    "-   `level1` is the finest taxonomic level possible. In the example above, we would include `hooded` and `no hood` but discard all images in `sweater` to avoid confusion; and proceed in the same manner for other classes.\n",
    "\n",
    "-   `level2` is a grouping of underlying levels. In the example above, it would include `shirt` (which contains all images in `shirt`, `long sleeves`, and `short sleeves`), `sweater` (which, similarly would include this class and all its children), `pants` (including children), and `shorts`. So typically, `level2` contains more images (less discarding), sorted within fewer classes than `level1`, and may therefore be an easier classification problem.\n",
    "\n",
    "-   `level3` is an even broader grouping. Here it would be `top` vs `bottom`\n",
    "\n",
    "-   etc.\n",
    "\n",
    "In the Plankton Image dataset, the objects will be categorised based on a pre-defined 'level1' and 'level2'. You can opt to work on one of them, but we recommend you to work on `level2` because it is an easier classification problem.  \n",
    "\n",
    "#### * Data Structure\n",
    "\n",
    "    /mnt/datasets/plankton/flowcam/\n",
    "        meta.csv\n",
    "        taxo.csv\n",
    "        features_native.csv.gz\n",
    "        features_skimage.csv.gz\n",
    "        imgs.zip\n",
    "\n",
    "* `meta.csv` contains the index of images and their corresponding labels\n",
    "* `taxo.csv` defines the taxonomic tree and its potential groupings at various level. Note that, the information is also available in `meta.csv`. Therefore, the information in `taxo.csv` is probably useless, but at least it gives you a global view about taxonomy tree\n",
    "* `features_native.csv.gz` contain the morphological handcrafted features computed by ZooProcess. In fact, ZooProcess generates the region of interests (ROI) around each individual object from a original image of Plankton. In addition, it also computes a set of associated features measured on the object. These features are the ones contained in `features_native.csv.gz`\n",
    "* `features_skimage.csv.gz` contains the morphological features recomputed with skimage.measure.regionprops on the ROIs produced by ZooProcess.\n",
    "* `imgs.zip` contains a post-processed version of the original images. Images are named by `objid`.jpg\n",
    "\n",
    "#### * Attributes in meta.csv\n",
    "\n",
    "The file contains the image identifiers (objid) as well as the labels assigned to the images by human operators. Those are defined with various levels of precision:\n",
    "\n",
    "* <i>unique_name</i>: raw labels from operators\n",
    "* <i>level1</i>: cleaned, most detailed labels\n",
    "* <i>level2</i>: regrouped (coarser) labels\n",
    "* <i>lineage</i>: full taxonomic lineage of the class\n",
    "\n",
    "Some labels may be missing (coded ‘NA’) at a given level, meaning that the corresponding objects should be discarded for the classification at this level.\n",
    "\n",
    "#### * imgs.zip\n",
    "\n",
    "This zip archive contains an *imgs* folder that contains all the images in .jpg format. Do not extract this folder to disk! Instead you will be loading the images to memory. See the code below for a quick how-to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def extract_zip_to_memory(input_zip):\n",
    "    '''\n",
    "    This function extracts the images stored inside the given zip file.\n",
    "    It stores the result in a python dictionary.\n",
    "    \n",
    "    input_zip (string): path to the zip file\n",
    "    \n",
    "    returns (dict): {filename (string): image_file (bytes)}\n",
    "    '''\n",
    "    input_zip=zipfile.ZipFile(input_zip)\n",
    "    return {name: BytesIO(input_zip.read(name)) for name in input_zip.namelist() if name.endswith('.jpg')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/mnt/datasets/plankton/flowcam/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_files = extract_zip_to_memory(base + \"imgs.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=83x156 at 0x7FADB52C4320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from PIL.ImageOps import fit\n",
    "# Display an example image \n",
    "# img = Image.open(img_files['imgs/32738710.jpg'])\n",
    "im = Image.open(img_files['imgs/32738711.jpg'])\n",
    "display(im)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objid</th>\n",
       "      <th>projid</th>\n",
       "      <th>id</th>\n",
       "      <th>status</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>objdate</th>\n",
       "      <th>objtime</th>\n",
       "      <th>depth_min</th>\n",
       "      <th>depth_max</th>\n",
       "      <th>unique_name</th>\n",
       "      <th>lineage</th>\n",
       "      <th>level1</th>\n",
       "      <th>level2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32756761.0</td>\n",
       "      <td>133</td>\n",
       "      <td>84963</td>\n",
       "      <td>V</td>\n",
       "      <td>43.683333</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2013-09-19</td>\n",
       "      <td>00:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>detritus</td>\n",
       "      <td>/#/not-living/detritus</td>\n",
       "      <td>detritus</td>\n",
       "      <td>detritus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32759364.0</td>\n",
       "      <td>133</td>\n",
       "      <td>84963</td>\n",
       "      <td>V</td>\n",
       "      <td>43.683333</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2013-09-19</td>\n",
       "      <td>00:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>detritus</td>\n",
       "      <td>/#/not-living/detritus</td>\n",
       "      <td>detritus</td>\n",
       "      <td>detritus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32758055.0</td>\n",
       "      <td>133</td>\n",
       "      <td>28299</td>\n",
       "      <td>V</td>\n",
       "      <td>43.683333</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2013-09-19</td>\n",
       "      <td>00:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>Guinardia</td>\n",
       "      <td>/#/living/Eukaryota/Harosa/Stramenopiles/Ochro...</td>\n",
       "      <td>Guinardia</td>\n",
       "      <td>Rhizosolenids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32758988.0</td>\n",
       "      <td>133</td>\n",
       "      <td>92010</td>\n",
       "      <td>V</td>\n",
       "      <td>43.683333</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2013-09-19</td>\n",
       "      <td>00:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>silks</td>\n",
       "      <td>/#/not-living/plastic/other/silks</td>\n",
       "      <td>silks</td>\n",
       "      <td>silks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32760598.0</td>\n",
       "      <td>133</td>\n",
       "      <td>92010</td>\n",
       "      <td>V</td>\n",
       "      <td>43.683333</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2013-09-19</td>\n",
       "      <td>00:09:00</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>silks</td>\n",
       "      <td>/#/not-living/plastic/other/silks</td>\n",
       "      <td>silks</td>\n",
       "      <td>silks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        objid  projid     id status   latitude  longitude     objdate  \\\n",
       "0  32756761.0     133  84963      V  43.683333        7.3  2013-09-19   \n",
       "1  32759364.0     133  84963      V  43.683333        7.3  2013-09-19   \n",
       "2  32758055.0     133  28299      V  43.683333        7.3  2013-09-19   \n",
       "3  32758988.0     133  92010      V  43.683333        7.3  2013-09-19   \n",
       "4  32760598.0     133  92010      V  43.683333        7.3  2013-09-19   \n",
       "\n",
       "    objtime  depth_min  depth_max unique_name  \\\n",
       "0  00:09:00          0         75    detritus   \n",
       "1  00:09:00          0         75    detritus   \n",
       "2  00:09:00          0         75   Guinardia   \n",
       "3  00:09:00          0         75       silks   \n",
       "4  00:09:00          0         75       silks   \n",
       "\n",
       "                                             lineage     level1         level2  \n",
       "0                             /#/not-living/detritus   detritus       detritus  \n",
       "1                             /#/not-living/detritus   detritus       detritus  \n",
       "2  /#/living/Eukaryota/Harosa/Stramenopiles/Ochro...  Guinardia  Rhizosolenids  \n",
       "3                  /#/not-living/plastic/other/silks      silks          silks  \n",
       "4                  /#/not-living/plastic/other/silks      silks          silks  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load the meta.csv\n",
    "df_meta = pd.read_csv(base + 'meta.csv')\n",
    "\n",
    "display(df_meta[:5])\n",
    "# df_meta.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>level1</th>\n",
       "      <td>3334</td>\n",
       "      <td>0.013686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level2</th>\n",
       "      <td>1003</td>\n",
       "      <td>0.004117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lineage</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Total   Percent\n",
       "level1    3334  0.013686\n",
       "level2    1003  0.004117\n",
       "lineage      0  0.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#missing data\n",
    "total = df_meta.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_meta.isnull().sum()/df_meta.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>level1</th>\n",
       "      <td>2618</td>\n",
       "      <td>0.010791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lineage</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Total   Percent\n",
       "level1    2618  0.010791\n",
       "level2       0  0.000000\n",
       "lineage      0  0.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta = df_meta.dropna(subset=['level2'])\n",
    "\n",
    "total = df_meta.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_meta.isnull().sum()/df_meta.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total n. of objects:  242607\n",
      "Total n. of labels:  39\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>level2</th>\n",
       "      <th>objid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>detritus</td>\n",
       "      <td>138439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>feces</td>\n",
       "      <td>26936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>Neoceratium</td>\n",
       "      <td>14014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>nauplii (Crustacea)</td>\n",
       "      <td>9293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>badfocus (artefact)</td>\n",
       "      <td>7848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>silks</td>\n",
       "      <td>5629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Copepoda</td>\n",
       "      <td>5141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>Thalassionema</td>\n",
       "      <td>5117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36</td>\n",
       "      <td>rods</td>\n",
       "      <td>4044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>multiple (other)</td>\n",
       "      <td>3261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>Codonellopsis (Dictyocystidae)</td>\n",
       "      <td>2888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>Protoperidinium</td>\n",
       "      <td>2256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23</td>\n",
       "      <td>Tintinnidiidae</td>\n",
       "      <td>2227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>Rhizosolenids</td>\n",
       "      <td>2160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>Chaetoceros</td>\n",
       "      <td>2105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>26</td>\n",
       "      <td>artefact</td>\n",
       "      <td>1849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>35</td>\n",
       "      <td>pollen</td>\n",
       "      <td>1821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>Codonaria</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>29</td>\n",
       "      <td>chainlarge</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24</td>\n",
       "      <td>Undellidae</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>egg (other)</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12</td>\n",
       "      <td>Hemiaulus</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>Dictyocysta</td>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11</td>\n",
       "      <td>Dinophysiales</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>Annelida</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>19</td>\n",
       "      <td>Rhabdonella</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21</td>\n",
       "      <td>Stenosemella</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>Coscinodiscids</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18</td>\n",
       "      <td>Retaria</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>16</td>\n",
       "      <td>Pleurosigma</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>Ceratocorys horrida</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>28</td>\n",
       "      <td>centric</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>15</td>\n",
       "      <td>Odontella (Mediophyceae)</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>Asterionellopsis</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9</td>\n",
       "      <td>Cyttarocylis</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>13</td>\n",
       "      <td>Lithodesmioides</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>tempChaetoceros danicus</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>25</td>\n",
       "      <td>Xystonellidae</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>Bacteriastrum</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                          level2   objid\n",
       "0      30                        detritus  138439\n",
       "1      32                           feces   26936\n",
       "2      14                     Neoceratium   14014\n",
       "3      34             nauplii (Crustacea)    9293\n",
       "4      27             badfocus (artefact)    7848\n",
       "5      37                           silks    5629\n",
       "6       7                        Copepoda    5141\n",
       "7      22                   Thalassionema    5117\n",
       "8      36                            rods    4044\n",
       "9      33                multiple (other)    3261\n",
       "10      6  Codonellopsis (Dictyocystidae)    2888\n",
       "11     17                 Protoperidinium    2256\n",
       "12     23                  Tintinnidiidae    2227\n",
       "13     20                   Rhizosolenids    2160\n",
       "14      4                     Chaetoceros    2105\n",
       "15     26                        artefact    1849\n",
       "16     35                          pollen    1821\n",
       "17      5                       Codonaria     845\n",
       "18     29                      chainlarge     751\n",
       "19     24                      Undellidae     710\n",
       "20     31                     egg (other)     685\n",
       "21     12                       Hemiaulus     670\n",
       "22     10                     Dictyocysta     549\n",
       "23     11                   Dinophysiales     525\n",
       "24      0                        Annelida     481\n",
       "25     19                     Rhabdonella     367\n",
       "26     21                    Stenosemella     357\n",
       "27      8                  Coscinodiscids     334\n",
       "28     18                         Retaria     257\n",
       "29     16                     Pleurosigma     191\n",
       "30      3             Ceratocorys horrida     186\n",
       "31     28                         centric     145\n",
       "32     15        Odontella (Mediophyceae)     131\n",
       "33      1                Asterionellopsis     117\n",
       "34      9                    Cyttarocylis     100\n",
       "35     13                 Lithodesmioides      68\n",
       "36     38         tempChaetoceros danicus      61\n",
       "37     25                   Xystonellidae      37\n",
       "38      2                   Bacteriastrum      12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uniqueMeta = len(list(set(df_meta['objid'])))\n",
    "print(\"Total n. of objects: \", uniqueMeta)\n",
    "\n",
    "uniqueLevel2 = len(list(set(df_meta['level2'])))\n",
    "print(\"Total n. of labels: \", uniqueLevel2)\n",
    "\n",
    "list_level2 = df_meta.groupby('level2', as_index=False).objid.count().sort_values(by=['objid'], ascending=False).reset_index()\n",
    "\n",
    "display(list_level2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to know how we should format the height x width image data dimensions\n",
    "# for inputting to a keras model\n",
    "def get_size_statistics():\n",
    "    heights = []\n",
    "    widths = []\n",
    "    img_count = 0\n",
    "    for i, j in df_meta.iterrows(): \n",
    "        img_id = j['objid']\n",
    "        img_path = 'imgs/' + str(int(img_id)) + '.jpg'\n",
    "        if img_path in img_files:\n",
    "            data = np.array(Image.open(img_files[img_path]))\n",
    "            heights.append(data.shape[0])\n",
    "            widths.append(data.shape[1])\n",
    "            img_count += 1\n",
    "        else:\n",
    "            df_meta.drop(index=i)\n",
    "            \n",
    "    avg_height = sum(heights) / len(heights)\n",
    "    avg_width = sum(widths) / len(widths)\n",
    "    print(\"Average Height: \" + str(avg_height))\n",
    "    print(\"Max Height: \" + str(max(heights)))\n",
    "    print(\"Min Height: \" + str(min(heights)))\n",
    "    print('\\n')\n",
    "    print(\"Average Width: \" + str(avg_width))\n",
    "    print(\"Max Width: \" + str(max(widths)))\n",
    "    print(\"Min Width: \" + str(min(widths)))\n",
    "\n",
    "# get_size_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total n. of objects:  242607\n"
     ]
    }
   ],
   "source": [
    "no_target_obj = len(list(set(df_meta['objid'])))\n",
    "print(\"Total n. of objects: \", no_target_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242607, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "level2_mapping = list_level2[['index', 'level2']].values\n",
    "level2_mapping_dict = {}\n",
    "level2_mapping_dict = {i[1] : i[0] for i in level2_mapping}\n",
    "\n",
    "df_meta['level2_mapping'] = df_meta['level2'].replace(level2_mapping_dict)\n",
    "target_data = df_meta[['objid', 'level2_mapping']].values\n",
    "print(target_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_FILEPATH = \"small_test_data.npy\"\n",
    "TRAIN_DATA_FILEPATH = \"small_train_data.npy\"\n",
    "VAL_DATA_FILEPATH = \"small_val_data.npy\"\n",
    "IMG_SIZE = 98\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 39\n",
    "RATIO = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31052, 2)\n",
      "(7764, 2)\n",
      "(9705, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# trainData, testData = train_test_split(train_data[0:128], train_size=0.6, test_size=0.4)\n",
    "small_target_data, rest_small_target_data = train_test_split(target_data, train_size=0.2, test_size=0.8, stratify=target_data[:,1])\n",
    "train_data, test_data = train_test_split(small_target_data, train_size=0.8, test_size=0.2, stratify=small_target_data[:,1])\n",
    "train_data, val_data = train_test_split(train_data, train_size=0.8, test_size=0.2, stratify=train_data[:,1])\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "np.save(TEST_DATA_FILEPATH, test_data)\n",
    "np.save(TRAIN_DATA_FILEPATH, train_data)\n",
    "np.save(VAL_DATA_FILEPATH, val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31052, 2)\n",
      "(7764, 2)\n",
      "(9705, 2)\n",
      "(31052,)\n",
      "(7764,)\n",
      "(9705,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)\n",
    "# print(train_data[0:5,0])\n",
    "print(np.unique(train_data[:,0]).shape)\n",
    "print(np.unique(val_data[:,0]).shape)\n",
    "print(np.unique(test_data[:,0]).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_image(img_id, target_size=IMG_SIZE):\n",
    "#     img_path = 'imgs/' + str(int(img_id)) + '.jpg'\n",
    "#     if img_path in img_files:\n",
    "#         img = Image.open(img_files['imgs/' + str(int(img_id)) + '.jpg'])\n",
    "#         img = img.convert('L')\n",
    "#         img_width, img_height = im.size\n",
    "#         new_img_size = (int(img_width*RATIO), int(img_height*RATIO))\n",
    "#         if(min(new_img_size) > target_size):\n",
    "#             new_im = img.resize((target_size,target_size), Image.ANTIALIAS)\n",
    "#         else:\n",
    "#             img = img.resize(new_img_size, Image.ANTIALIAS)\n",
    "#             new_im = Image.new(\"L\", (target_size, target_size), 255)\n",
    "#             new_im.paste(img, ((target_size - new_img_size[0])//2,\n",
    "#                     (target_size - new_img_size[1])//2))\n",
    "#         img_array = np.array(new_im)\n",
    "#         img_array = img_array.reshape(img_array.shape + (1,))\n",
    "#     return img_array \n",
    "\n",
    "# test = load_image(32738711)\n",
    "# print(test)\n",
    "# plt.imshow(test)\n",
    "\n",
    "def load_image(img_id, target_size=(IMG_SIZE, IMG_SIZE)):\n",
    "    img_path = 'imgs/' + str(int(img_id)) + '.jpg'\n",
    "    if img_path in img_files:\n",
    "        img = Image.open(img_files['imgs/' + str(int(img_id)) + '.jpg'])\n",
    "        img = img.convert('L')\n",
    "        img = img.resize(target_size, Image.ANTIALIAS)\n",
    "        img = np.array(img)\n",
    "        img = img.reshape(img.shape + (1,))\n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# train_data_gen = ImageDataGenerator(\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.1,\n",
    "#     height_shift_range=0.1,\n",
    "#     horizontal_flip=True,\n",
    "# )\n",
    "\n",
    "# generator function to return images batchwise\n",
    "def generator(data, augment=False, batch_size=BATCH_SIZE):\n",
    "    while True:\n",
    "        # Randomize the indices to make an array\n",
    "        suffer_data = np.random.permutation(data)\n",
    "        for batch in range(0, len(suffer_data), batch_size):\n",
    "            # slice out the current batch according to batch-size\n",
    "            current_batch = suffer_data[batch:(batch + batch_size)]\n",
    "\n",
    "            x_train = np.array(list(map(load_image, current_batch[:,0])))\n",
    "            y_train = to_categorical(current_batch[:,1], num_classes=NUM_CLASSES)\n",
    "            x_train = x_train.astype('float32') / 255\n",
    "            yield (x_train, y_train)\n",
    "#             if(augment == True):\n",
    "#                 x_batch_train, y_batch_train = train_data_gen.flow(x_train, y_train, batch_size=batch_size).next()\n",
    "#                 yield (x_batch_train, y_batch_train)\n",
    "#             else:\n",
    "#                 yield (x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        print(\"test:\", self.validation_data.dtype)\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        print(\" — val_f1: %f \" %(_val_f1))\n",
    "        return\n",
    "\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "# checkpoint\n",
    "WEIGHT_FILEPATH = \"exp.weights.best.hdf5\"\n",
    "OUTPUT_FILEPATH = \"exp.output.log\"\n",
    "checkpoint = ModelCheckpoint(WEIGHT_FILEPATH, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "csvlog = CSVLogger(OUTPUT_FILEPATH, separator=',', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers. normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "# # model.add(LeakyReLU(alpha=0.1))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "# # model.add(LeakyReLU(alpha=0.1))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "# # model.add(LeakyReLU(alpha=0.1))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "# # model.add(LeakyReLU(alpha=0.1))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# # model.add(LeakyReLU(alpha=0.1))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# # model.add(LeakyReLU(alpha=0.1))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(NUM_CLASSES, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(NUM_CLASSES, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 96, 96, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 46, 46, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 21, 21, 96)        55392     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 10, 10, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 128)         110720    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 39)                5031      \n",
      "=================================================================\n",
      "Total params: 468,743\n",
      "Trainable params: 468,743\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and weights for continue training\n",
    "model.load_weights(WEIGHT_FILEPATH)\n",
    "\n",
    "test_data = np.load(TEST_DATA_FILEPATH)\n",
    "train_data = np.load(TRAIN_DATA_FILEPATH)\n",
    "val_data = np.load(VAL_DATA_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = int(np.ceil(len(train_data) / float(BATCH_SIZE)))\n",
    "validation_steps = int(np.ceil(len(val_data) / float(BATCH_SIZE)))\n",
    "test_steps = int(np.ceil(len(test_data) / float(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "971/971 [==============================] - 2865s 3s/step - loss: 1.2025 - acc: 0.6454 - val_loss: 0.9855 - val_acc: 0.6816\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.75465\n",
      "Epoch 2/10\n",
      "587/971 [=================>............] - ETA: 17:18 - loss: 0.8275 - acc: 0.7268"
     ]
    }
   ],
   "source": [
    "#final run n1\n",
    "#with augaument\n",
    "model.fit_generator(generator(train_data, True), steps_per_epoch = steps_per_epoch, epochs = 10, verbose = 1,\n",
    "                   validation_data=generator(val_data), validation_steps = validation_steps, callbacks=[checkpoint,csvlog])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1214/1214 [==============================] - 1033s 851ms/step - loss: 0.5923 - acc: 0.7941 - val_loss: 0.6523 - val_acc: 0.7840\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.78401, saving model to weights.best.hdf5\n",
      "Epoch 2/20\n",
      " 224/1214 [====>.........................] - ETA: 14:19 - loss: 0.5849 - acc: 0.7939"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-f608d88f98bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#with augaument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit_generator(generator(train_data, True), steps_per_epoch = steps_per_epoch, epochs = 20, verbose = 1,\n\u001b[0;32m----> 4\u001b[0;31m                    validation_data=generator(val_data), validation_steps = validation_steps, callbacks=[checkpoint])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#final run n1 old\n",
    "#with augaument\n",
    "model.fit_generator(generator(train_data, True), steps_per_epoch = steps_per_epoch, epochs = 20, verbose = 1,\n",
    "                   validation_data=generator(val_data), validation_steps = validation_steps, callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(model.history.history['acc'])\n",
    "plt.plot(model.history.history['val_acc'])\n",
    "plt.title('model f1 accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(model.history.history['loss'])\n",
    "plt.plot(model.history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190/190 [==============================] - 78s 413ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7457601272687884, 0.7493095915721729]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.evaluate_generator(generator(test_data),steps=test_steps,verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24261, 98, 98, 1) (24261, 39)\n"
     ]
    }
   ],
   "source": [
    "test_generator = generator(test_data,False,batch_size=test_data.shape[0])\n",
    "x_test, y_test_true = next(test_generator)\n",
    "print(x_test.shape, y_test_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24261/24261 [==============================] - 87s 4ms/step\n",
      "(24261, 39)\n"
     ]
    }
   ],
   "source": [
    "y_test_predict = model.predict(x_test, verbose=1)\n",
    "print(y_test_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30 30 32 14 30]\n",
      "[30 30 30 14 30]\n",
      "score: 0.3084183548366016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# print(y_test_true.dtype)\n",
    "# print(y_test_predict.dtype)\n",
    "\n",
    "y_true = np.argmax(y_test_true, axis=1)\n",
    "y_pred = np.argmax(y_test_predict, axis=1)\n",
    "print(y_true[:5])\n",
    "print(y_pred[:5])\n",
    "score = f1_score(y_true, y_pred, average='macro') \n",
    "print(\"score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * my note\n",
    "A brief outline of the availabel attributes in `features_native.csv.gz` which you can use is given below:\n",
    "\n",
    "* <i>epoch</i>: only 1 epoch is not enough: underfitting -> optimal -> overfitting\n",
    "* <i>batch_size</i>: should increase?\n",
    "* <i>f1 score</i>: https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/\n",
    "* <i>f1 score</i>: https://mikulskibartosz.name/f1-score-explained-d94ee90dec5b\n",
    "* <i>stddev</i>: standard deviation of greys\n",
    "* <i>min</i>: minimum grey\n",
    "* <i>perim.</i>: perimeter of ROI\n",
    "* <i>width, height</i>: dimensions of ROI\n",
    "* <i>major, minor</i>: length of major,minor axis of the best fitting ellipse\n",
    "* <i>angle</i>: \n",
    "* <i>circ.</i>: circularity or shape factor which can be computed by 4pi(area/perim.^2)\n",
    "* <i>feret</i>:  maximal feret diameter\n",
    "* <i>intden</i>: integrated density: mean*area\n",
    "* <i>median</i>: median grey\n",
    "* <i>skew, kurt</i>: skewness,kurtosis of the histogram of greys\n",
    "* <i>%area</i>: proportion of the image corresponding to the object\n",
    "* <i>area_exc</i>: area excluding holes\n",
    "* <i>fractal</i>: fractal dimension of the perimeter\n",
    "* <i>skelarea</i>: area of the one-pixel wide skeleton of the image ???\n",
    "* <i>slope</i>: slope of the cumulated histogram of greys\n",
    "* <i>histcum1, 2, 3</i>:  grey level at quantiles 0.25, 0.5, 0.75 of the histogram of greys\n",
    "* <i>nb1, 2, 3</i>: number of objects after thresholding at the grey levels above\n",
    "* <i>symetrieh, symetriev</i>: index of horizontal,vertical symmetry\n",
    "* <i>symetriehc, symetrievc</i>: same but after thresholding at level histcum1\n",
    "* <i>convperim, convarea</i>: perimeter,area of the convex hull of the object\n",
    "* <i>fcons</i>: contrast\n",
    "* <i>thickr</i>: thickness ratio: maximum thickness/mean thickness\n",
    "* <i>esd</i>:\n",
    "* <i>elongation</i>: elongation index: major/minor\n",
    "* <i>range</i>: range of greys: max-min\n",
    "* <i>meanpos</i>:  relative position of the mean grey: (max-mean)/range\n",
    "* <i>centroids</i>:\n",
    "* <i>cv</i>: coefficient of variation of greys: 100*(stddev/mean)\n",
    "* <i>sr</i>: index of variation of greys: 100*(stddev/range)\n",
    "* <i>perimareaexc</i>:\n",
    "* <i>feretareaexc</i>:\n",
    "* <i>perimferet</i>: index of the relative complexity of the perimeter: perim/feret\n",
    "* <i>perimmajor</i>: index of the relative complexity of the perimeter: perim/major\n",
    "* <i>circex</i>:\n",
    "* <i>cdexc</i>:\n",
    "* <i>kurt_mean</i>:\n",
    "* <i>skew_mean</i>:\n",
    "* <i>convperim_perim</i>:\n",
    "* <i>convarea_area</i>:\n",
    "* <i>symetrieh_area</i>:\n",
    "* <i>symetriev_area</i>:\n",
    "* <i>nb1_area</i>:\n",
    "* <i>nb2_area</i>:\n",
    "* <i>nb3_area</i>:\n",
    "* <i>nb1_range</i>:\n",
    "* <i>nb2_range</i>:\n",
    "* <i>nb3_range</i>:\n",
    "* <i>median_mean</i>:\n",
    "* <i>median_mean_range</i>:\n",
    "* <i>skeleton_area</i>:\n",
    "\n",
    "#### * data exploriation\n",
    "- number of object in meta\n",
    "- number of label in meta\n",
    "- number of image in imgs\n",
    "- after mapping img - label: check the number of pairs (obj - label) again\n",
    "- if different ==> why?? unidentified? should take it as a separate class?\n",
    "- the distribution of label: imbalanced dataset? \n",
    "\n",
    "- img dimension (1 or 3 channel?)\n",
    "- img size and its distribution \n",
    "- draw 5 imgs for each of 10 level2\n",
    "\n",
    "#### * model selection\n",
    "- convnet: definition + advantages + disadvantages\n",
    "- any solution for disadvantages if any?\n",
    "- why it is suitable in this problem?\n",
    "- choose layer and explain\n",
    "- https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/\n",
    "\n",
    "#### * model improvement\n",
    "- custom method for img resize?\n",
    "- drop out? what is it? its advantages?\n",
    "- augement data: rotating, zooming, mirroring, blurring or shearing\n",
    "- slow >> change size of batch?\n",
    "- using LReLu?\n",
    "- set class weights for imbalanced classes?\n",
    "-  Transfer Learning? Ensemble Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Attributes in features_native.csv.gz\n",
    "A brief outline of the availabel attributes in `features_native.csv.gz` which you can use is given below:\n",
    "\n",
    "* <i>objid</i>: same as in `meta.csv`\n",
    "* <i>area</i>: area of ROI\n",
    "* <i>meanimagegrey</i>:\n",
    "* <i>mean</i>: mean grey\n",
    "* <i>stddev</i>: standard deviation of greys\n",
    "* <i>min</i>: minimum grey\n",
    "* <i>perim.</i>: perimeter of ROI\n",
    "* <i>width, height</i>: dimensions of ROI\n",
    "* <i>major, minor</i>: length of major,minor axis of the best fitting ellipse\n",
    "* <i>angle</i>: \n",
    "* <i>circ.</i>: circularity or shape factor which can be computed by 4pi(area/perim.^2)\n",
    "* <i>feret</i>:  maximal feret diameter\n",
    "* <i>intden</i>: integrated density: mean*area\n",
    "* <i>median</i>: median grey\n",
    "* <i>skew, kurt</i>: skewness,kurtosis of the histogram of greys\n",
    "* <i>%area</i>: proportion of the image corresponding to the object\n",
    "* <i>area_exc</i>: area excluding holes\n",
    "* <i>fractal</i>: fractal dimension of the perimeter\n",
    "* <i>skelarea</i>: area of the one-pixel wide skeleton of the image ???\n",
    "* <i>slope</i>: slope of the cumulated histogram of greys\n",
    "* <i>histcum1, 2, 3</i>:  grey level at quantiles 0.25, 0.5, 0.75 of the histogram of greys\n",
    "* <i>nb1, 2, 3</i>: number of objects after thresholding at the grey levels above\n",
    "* <i>symetrieh, symetriev</i>: index of horizontal,vertical symmetry\n",
    "* <i>symetriehc, symetrievc</i>: same but after thresholding at level histcum1\n",
    "* <i>convperim, convarea</i>: perimeter,area of the convex hull of the object\n",
    "* <i>fcons</i>: contrast\n",
    "* <i>thickr</i>: thickness ratio: maximum thickness/mean thickness\n",
    "* <i>esd</i>:\n",
    "* <i>elongation</i>: elongation index: major/minor\n",
    "* <i>range</i>: range of greys: max-min\n",
    "* <i>meanpos</i>:  relative position of the mean grey: (max-mean)/range\n",
    "* <i>centroids</i>:\n",
    "* <i>cv</i>: coefficient of variation of greys: 100*(stddev/mean)\n",
    "* <i>sr</i>: index of variation of greys: 100*(stddev/range)\n",
    "* <i>perimareaexc</i>:\n",
    "* <i>feretareaexc</i>:\n",
    "* <i>perimferet</i>: index of the relative complexity of the perimeter: perim/feret\n",
    "* <i>perimmajor</i>: index of the relative complexity of the perimeter: perim/major\n",
    "* <i>circex</i>:\n",
    "* <i>cdexc</i>:\n",
    "* <i>kurt_mean</i>:\n",
    "* <i>skew_mean</i>:\n",
    "* <i>convperim_perim</i>:\n",
    "* <i>convarea_area</i>:\n",
    "* <i>symetrieh_area</i>:\n",
    "* <i>symetriev_area</i>:\n",
    "* <i>nb1_area</i>:\n",
    "* <i>nb2_area</i>:\n",
    "* <i>nb3_area</i>:\n",
    "* <i>nb1_range</i>:\n",
    "* <i>nb2_range</i>:\n",
    "* <i>nb3_range</i>:\n",
    "* <i>median_mean</i>:\n",
    "* <i>median_mean_range</i>:\n",
    "* <i>skeleton_area</i>:\n",
    "\n",
    "#### * Attributes in features_skimage.csv.gz\n",
    "Table of morphological features recomputed with skimage.measure.regionprops on the ROIs produced by ZooProcess. See http://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.regionprops for documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the size of an organism in the resulting images is proportional to its actual size, and does not depend on the distance to the lens of the camera. This means that size carries useful information for the task of identifying the species. In practice it also means that all the images in the dataset have different sizes.\n",
    "\n",
    "These predicted distributions were scored using the log loss (which corresponds to the negative log likelihood or equivalently the cross-entropy loss)\n",
    "\n",
    "Image classification problems are often approached using convolutional neural networks these days, and with good reason: they achieve record-breaking performance on some really difficult tasks.\n",
    "\n",
    "Most importantly, just like images of galaxies, images of plankton are (mostly) rotation invariant. I used this property for data augmentation, and incorporated it into the model architecture.\n",
    "\n",
    "Pre-processing and data augmentation: rescaling the images in various ways and then performing global zero mean unit variance (ZMUV) normalization, to improve the stability of training and increase the convergence speed.\n",
    "\n",
    "Data augmentation: \n",
    "rotation: random with angle between 0° and 360° (uniform)\n",
    "translation: random with shift between -10 and 10 pixels (uniform)\n",
    "rescaling: random with scale factor between 1/1.6 and 1.6 (log-uniform)\n",
    "flipping: yes or no (bernoulli)\n",
    "shearing: random with angle between -20° and 20° (uniform)\n",
    "stretching: random with stretch factor between 1/1.3 and 1.3 (log-uniform)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
